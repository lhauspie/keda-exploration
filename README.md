# Exploration de Keda

Mon objectif est d'explorer Keda comme orchestrateur de mes Batches sur Kubernetes.

Keda est une solution de scaling en fonction d'autre chose que ce qu'est capable le Horizontal Pod Autoscaling : https://keda.sh/

On peut donc scale nos pods en fonction d'une quantit√© de lignes dans une table postgres par exemple ou encore en fonction d'un topic Kafka.
La liste est assez compl√®te, je vous invite √† consulter la liste des "scalers" ici : https://keda.sh/docs/2.3/scalers


## Pr√©-requis

Puisque je veux faire mes exp√©riences uniquement en local afin de m'affranchir de tout provider et permettre au plus grand nombre reproduire cette exploration en local,
il nous faut installer toute une batterie d'outils pour mener √† bien cette exp√©rience.

Voici la liste des outils √† installer :
- minikube : https://minikube.sigs.k8s.io/docs/start/
- kubectl : https://kubernetes.io/docs/tasks/tools
- helm : https://helm.sh/docs/intro/install/

Les docs d'installation √©tant ultra bien faites, je vous laisse explorer ces diff√©rentes ressources pour faire votre propre install.

### Quelques commandes utiles

#### Minikube

Lancer le cluster Kubernetes minikube :
```zsh
$ minikube start
üòÑ  minikube v1.22.0 sur Darwin 11.5.1
‚ú®  Utilisation du pilote docker bas√© sur le profil existant
üëç  D√©marrage du noeud de plan de contr√¥le minikube dans le cluster minikube
üöú  Extraction de l'image de base...
üî•  Cr√©ation de docker container (CPUs=2, Memory=1987Mo) ...
üê≥  Pr√©paration de Kubernetes v1.21.2 sur Docker 20.10.7...
üîé  V√©rification des composants Kubernetes...
    ‚ñ™ Utilisation de l'image kubernetesui/dashboard:v2.1.0
    ‚ñ™ Utilisation de l'image kubernetesui/metrics-scraper:v1.0.4
    ‚ñ™ Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Modules activ√©s: storage-provisioner, default-storageclass, dashboard
üèÑ  Termin√© ! kubectl est maintenant configur√© pour utiliser "minikube" cluster et espace de noms "default" par d√©faut.
```

Lancer l'interface web :
```zsh
$ minikube dashboard
ü§î  V√©rification de l'√©tat du tableau de bord...
üöÄ  Lancement du proxy...
ü§î  V√©rification de l'√©tat du proxy...
üéâ  Ouverture de http://127.0.0.1:51094/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ dans votre navigateur par d√©faut...
```

Arr√™ter minikube :
```zsh
$ minikube stop
‚úã  N≈ìud d'arr√™t "minikube" ...
üõë  Mise hors tension du profil "minikube" via SSH‚Ä¶
üõë  1 n≈ìud(s) arr√™t√©(s).
```

#### Helm

Il est possible de lister l'ensemble des charts helm d√©ploy√©s sur notre cluster :
```zsh
$ helm list --all-namespaces
NAME            NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
keda            keda                    1               2021-07-30 23:19:20.030734 +0200 CEST   deployed        keda-2.3.2              2.3.0      
my-release      keda-with-postgresql    1               2021-07-31 00:09:29.392879 +0200 CEST   deployed        postgresql-10.8.0       11.12.0    
```

#### Kubectl

Connaitre l'√©tat globale du cluster Kubernetes :
```zsh
$ kubectl get all --all-namespaces
NAMESPACE              NAME                                             READY   STATUS    RESTARTS   AGE
kube-system            pod/coredns-558bd4d5db-7ghq5                     1/1     Running   1          63m
kube-system            pod/etcd-minikube                                1/1     Running   1          63m
kube-system            pod/kube-apiserver-minikube                      1/1     Running   1          63m
kube-system            pod/kube-controller-manager-minikube             1/1     Running   1          63m
kube-system            pod/kube-proxy-pf9bk                             1/1     Running   1          63m
kube-system            pod/kube-scheduler-minikube                      1/1     Running   1          63m
kube-system            pod/storage-provisioner                          1/1     Running   3          63m
kubernetes-dashboard   pod/dashboard-metrics-scraper-7976b667d4-gdl4k   1/1     Running   1          62m
kubernetes-dashboard   pod/kubernetes-dashboard-6fcdf4f6d-rwf29         1/1     Running   2          62m

NAMESPACE              NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE
default                service/kubernetes                  ClusterIP   10.96.0.1      <none>        443/TCP                  63m
kube-system            service/kube-dns                    ClusterIP   10.96.0.10     <none>        53/UDP,53/TCP,9153/TCP   63m
kubernetes-dashboard   service/dashboard-metrics-scraper   ClusterIP   10.110.8.49    <none>        8000/TCP                 62m
kubernetes-dashboard   service/kubernetes-dashboard        ClusterIP   10.96.228.58   <none>        80/TCP                   62m

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   63m

NAMESPACE              NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
kube-system            deployment.apps/coredns                     1/1     1            1           63m
kubernetes-dashboard   deployment.apps/dashboard-metrics-scraper   1/1     1            1           62m
kubernetes-dashboard   deployment.apps/kubernetes-dashboard        1/1     1            1           62m

NAMESPACE              NAME                                                   DESIRED   CURRENT   READY   AGE
kube-system            replicaset.apps/coredns-558bd4d5db                     1         1         1       63m
kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-7976b667d4   1         1         1       62m
kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-6fcdf4f6d         1         1         1       62m
```


## D√©ploiement de Keda

On va commencer par d√©ployer Keda sur notre cluster.

La documentation est l√† aussi assez bien faite avec des modes d'installation alternatifs sur base de helm, kubectl ou directement via le "Operator Hub" : https://keda.sh/docs/2.3/deploy/

Je fais le choix de helm pour pouvoir revenir en arri√®re si besoin pour recommencer :
```zsh
$ helm repo add kedacore https://kedacore.github.io/charts
"kedacore" has been added to your repositories

$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
Update Complete. ‚éàHappy Helming!‚éà

$ kubectl create namespace keda
namespace/keda created

$ helm install keda kedacore/keda --namespace keda
NAME: keda
LAST DEPLOYED: Fri Jul 30 23:19:20 2021
NAMESPACE: keda
STATUS: deployed
REVISION: 1
TEST SUITE: None
```

Ok, il semblerait que tout ce soit bien pass√©, on va v√©rifier l'√©tat du cluster pour comprendre ce qu'il s'est pass√© :
On constate un nouveau namespace nomm√© keda...
```zsh
$ kubectl get namespaces
NAME                   STATUS   AGE
default                Active   98m
keda                   Active   2m32s
kube-node-lease        Active   98m
kube-public            Active   98m
kube-system            Active   98m
kubernetes-dashboard   Active   97m
```
... Ainsi qu'un ensemble de composants install√©s √† l'int√©rieur.
```zsh
$ kubectl get all --namespace keda
NAME                                                   READY   STATUS    RESTARTS   AGE
pod/keda-operator-7fc5699d47-q6zxj                     1/1     Running   0          6m40s
pod/keda-operator-metrics-apiserver-57fc85685f-pqfrd   1/1     Running   0          6m40s

NAME                                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/keda-operator-metrics-apiserver   ClusterIP   10.109.12.30   <none>        443/TCP,80/TCP   6m40s

NAME                                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/keda-operator                     1/1     1            1           6m40s
deployment.apps/keda-operator-metrics-apiserver   1/1     1            1           6m40s

NAME                                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/keda-operator-7fc5699d47                     1         1         1       6m40s
replicaset.apps/keda-operator-metrics-apiserver-57fc85685f   1         1         1       6m40s
```

Ces diff√©rents composants vont nous permettre de d√©ployer de nouveau composants nomm√©s [ScaledObject](https://keda.sh/docs/2.3/concepts/scaling-deployments/) et [ScaledJob](https://keda.sh/docs/2.3/concepts/scaling-jobs/) qui vont eux piloter l'autoscaling des pods qu'ils soient des "Deployments", des "StatefulSets", des "Custom Resources" ou des "Jobs".

Et celui qui nous int√©resse aujourd'hui est l'autoscaling des "Jobs" pour pouvoir traiter potentiellement un nombre tr√®s variable de Batches en parall√®le.

Petit commande bien utile pour d√©bugger quand Keda n'arrive pas √† faire son office :
```zsh
$ kubectl logs -f -n keda keda-operator-7fc5699d47-q6zxj -c keda-operator
```

## Passons aux choses s√©rieuses

Pour mon premier essaye avec Keda, je choisi d'utiliser le scaler PostgreSQL.
Je vais donc tout r√©aliser dans un namespace d√©di√© :
```zsh
$ kubectl create namespace keda-with-postgresql
namespace/keda-with-postgresql created

$ kubectl config set-context --current --namespace keda-with-postgresql
Context "minikube" modified.
```

Tout le d√©roulement qui suit se fera depuis le dossier postgres-batch :
```zsh
$ cd postrgres-batch
```

### D√©ploiement de PostgreSQL sur mon cluster minikube

Je n'ai pas trop envie de me prendre la t√™te √† tout √©crire moi-m√™me pour d√©ployer PostgreSQL sur mon cluster donc je pr√©f√®re passer par les charts helm de bitnami : https://github.com/bitnami/charts

```zsh
$ helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories

$ helm install my-release bitnami/postgresql
NAME: my-release
LAST DEPLOYED: Sat Jul 31 00:09:29 2021
NAMESPACE: keda-with-postgresql
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
** Please be patient while the chart is being deployed **

PostgreSQL can be accessed via port 5432 on the following DNS names from within your cluster:

    my-release-postgresql.keda-with-postgresql.svc.cluster.local - Read/Write connection

To get the password for "postgres" run:

    export POSTGRES_PASSWORD=$(kubectl get secret --namespace keda-with-postgresql my-release-postgresql -o jsonpath="{.data.postgresql-password}" | base64 --decode)

To connect to your database run the following command:

    kubectl run my-release-postgresql-client --rm --tty -i --restart='Never' --namespace keda-with-postgresql --image docker.io/bitnami/postgresql:11.12.0-debian-10-r44 --env="PGPASSWORD=$POSTGRES_PASSWORD" --command -- psql --host my-release-postgresql -U postgres -d postgres -p 5432

To connect to your database from outside the cluster execute the following commands:

    kubectl port-forward --namespace keda-with-postgresql svc/my-release-postgresql 5432:5432 &
    PGPASSWORD="$POSTGRES_PASSWORD" psql --host 127.0.0.1 -U postgres -d postgres -p 5432
```

Sympa le chart, il nous indique m√™me comment on se connect √† base donn√©es...

### Fabrication du batch

Pour l'exemple, on va faire un Job tr√®s simple.
Il aura la lourde t√¢che d'ajouter une ligne au status "RUNNING" dans une table `clap` de la base de donn√©es puis d'attendre pendant X temps puis de changer le status de cette ligne en "SUCCESS" comme pourrait le faire un batch classique.

Pour les besoins du test de Keda avec le Scaler PostgreSQL, je vais aussi cr√©er une table `make_it_clap` pour "piloter" le scaling.
Elle contiendra une colonne `duration_seconds` qui permettra de sp√©cifier le temps d'un `clap` pour bien voir le scaling se mettre en place.

Cette premi√®re version est disponible dans [v1/make-it-clap.sql](postgres-batch/scripts/v1/make-it-clap.sql).

√âtant donn√© que je souhaite que tout se fasse en local, je ne veux pas avoir √† push mon image docker sur une registry publique.
Pour √©viter √ßa, il est possible de faire en sorte que notre shell pointe sur le deamon docker de minikube.
Aisni, lors d'un `docker build`, l'image r√©sulante ira dans le cache locale de minikube, ce qui lui permettra d'utiliser l'image docker sans faire de pull.

Il faut donc executer les commandes suivantes :
```zsh
$ eval $(minikube docker-env)
$ docker build . -t postgres-batch:latest
```

Pour revenir au deamon docker local, executer simplement :
```zsh
$ eval $(minikube docker-env -u)
```

Voil√†, notre image docker est maintenant accessible depuis minikube

### Test du batch

Pour tester ce batch, il faut :
- Ins√©rer une ligne dans la table `make_it_clap` : `INSERT INTO make_it_clap (duration_seconds) VALUES (40);`
- D√©ployer manuellement le `Job` : `$ kubectl apply -f job.yaml`
- V√©rifier que le job est en cours : `kubectl get jobs `
```
NAME             COMPLETIONS   DURATION   AGE
postgres-batch   0/1           12s        121m
```
- V√©rifier que le job est termin√© : `kubectl get jobs `
```
NAME             COMPLETIONS   DURATION   AGE
postgres-batch   1/1           42s        121m
```
- Controller que le batch s'est correctement d√©roul√© : `SELECT * FROM clap;`
```
 id | status  | session_id | creation_date | creation_time   |    end_time
----+---------+------------+---------------+-----------------+-----------------
  1 | SUCCESS |   49918852 | 2021-08-02    | 12:10:23.14951  | 12:11:03.751734
(1 row)
```

## Passage √† l'√©chelle avec Keda

Il est maintenant l'heure de faire passer notre Job √† l'√©chelle en d√©l√©guant le pilotage du scaling √† Keda.

Keda permet de piloter le nombre de `Jobs` √† executer sur base d'une requ√™te SQL execut√©e √† interval de temps r√©gulier.

Voici le descripteur de `ScaledJob` utilis√© ici dans notre cas :
```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  namespace: keda-with-postgresql
  name: postgres-batch
spec:
  jobTargetRef:
    template:
      spec:
        containers:
          - name: postgres-batch
            image: postgres-batch:latest
            imagePullPolicy: Never
            env:
              - name: PG_HOST
                value: "my-release-postgresql"
              - name: PG_PORT
                value: "5432"
              - name: PG_USERNAME
                value: "postgres"
              - name: PGPASSWORD
                valueFrom:
                  secretKeyRef:
                    name: my-release-postgresql
                    key: postgresql-password
        restartPolicy: Never
    backoffLimit: 4
  pollingInterval: 30             # Optional. Default: 30 seconds
  maxReplicaCount: 10             # Optional. Default: 100
  successfulJobsHistoryLimit: 10  # Optional. Default: 100. How many completed jobs should be kept.
  failedJobsHistoryLimit: 5       # Optional. Default: 100. How many failed jobs should be kept.
  triggers:
    - type: postgresql
      metadata:
        userName: "postgres"
        passwordFromEnv: PGPASSWORD
        host: my-release-postgresql.keda-with-postgresql.svc.cluster.local #use the cluster-wide namespace as KEDA lives in a different namespace from your postgres
        port: "5432"
        dbName: postgres
        sslmode: disable
        query: "SELECT COUNT(1) FROM make_it_clap LEFT JOIN clap ON make_it_clap.id = clap.id WHERE clap.status IS NULL;"
        targetQueryValue: "1"
```

Toute la partie `spec.jobTargetRef.template` est strictement la m√™me que dans `job.yaml` donc on n'est pas perdu.
En revanche, la suite n√©cessite une petite explication :
- `pollingInterval` : interval de temps entre 2 execution de la requ√™te
- `maxReplicaCount` : nombre maximum de `Jobs` √† lancer en parall√®le
- `successfulJobsHistoryLimit` : nombre de `Jobs` en succ√®s √† garder dans l'historique
- `failedJobsHistoryLimit` : nombre de `Jobs` en en erreur √† garder dans l'historique

l'√©l√©ment le plus import de la partie `triggers` est la `query` qui servira √† connaitre le nombre de `Jobs` √† √©xecuter en parall√©le.

Dans notre exemple, chaque nouvelle ligne donnera naissance √† un nouveau `Job` dans la limite de `maxReplicaCount`.
Si on voulait quelque chose de moins radicale, on pourrait diviser le r√©sultat de la requ√™te par un entier :
```SQL
SELECT ceil(COUNT(1)::decimal / 10) FROM make_it_clap LEFT JOIN clap ON make_it_clap.id = clap.id WHERE clap.status IS NULL;
```
Ce qui r√©duirait par 10 le nombre de `Jobs` en parall√®le pour finir par les executer 1 √† 1.

Comme d'habitude, on d√©ploie le `ScaledJob` avec la commande :
```zsh
$ kubectl apply -f deploy/scaled-job.yaml
```

Ok, maintenant que le `ScaledJob` est d√©ploy√©, le principe est plut√¥t simple, on doit ins√©rer des lignes dans la table `make_it_clap` pour √©xecuter les Jobs :
```SQL
-- r√©p√©t√© 60 fois
INSERT INTO make_it_clap (duration_seconds) VALUES (40);
```

On peut surveiller l'√©volution des `Jobs` :
```zsh
$ kubectl get jobs
NAME                   COMPLETIONS   DURATION   AGE
postgres-batch-292ng   0/1           8s         10s
postgres-batch-2cfnw   0/1           8s         10s
[...]
postgres-batch-zklk2   0/1           10s        10s
```

ainsi que celle des `claps` :
```SQL
postgres=# select * from clap;
 id | status  | session_id | creation_date |  creation_time  |    end_time
----+---------+------------+---------------+-----------------+-----------------
  1 | RUNNING |  333443748 | 2021-08-02    | 16:17:41.28766  |
  2 | RUNNING |  637700789 | 2021-08-02    | 16:17:46.266496 |
[...]
 30 | RUNNING |  752837758 | 2021-08-02    | 16:18:29.902603 |
```

Au bout d'un certain temps, on voit que de nouveaux `Jobs` prennent la suite quand les premiers `Jobs` se terminent :
```zsh
$ kubectl get jobs.batch
NAME                   COMPLETIONS   DURATION   AGE
postgres-batch-292ng   1/1           86s        96s
postgres-batch-2cfnw   1/1           88s        96s
postgres-batch-fbhfz   0/1           5s         5s
[...]
postgres-batch-4jtnn   0/1           65s        65s
postgres-batch-4nc8p   0/1           5s         5s
```

ainsi que les `claps` qui s'executent :
```SQL
select * from clap ORDER BY id;
 id | status  | session_id | creation_date |  creation_time  |    end_time
----+---------+------------+---------------+-----------------+-----------------
  1 | SUCCESS |  333443748 | 2021-08-02    | 16:17:41.28766  | 16:18:21.839829
  2 | SUCCESS |  637700789 | 2021-08-02    | 16:17:46.266496 | 16:18:27.084621
  3 | SUCCESS |   45350398 | 2021-08-02    | 16:17:47.881485 | 16:18:28.256152
[...]
 11 | RUNNING |  596834214 | 2021-08-02    | 16:17:53.469475 | 
 12 | RUNNING |  154339869 | 2021-08-02    | 16:17:53.566302 | 
[...]
 31 | RUNNING |  752837758 | 2021-08-02    | 16:18:54.986443 | 
```

OK, on voit que √ßa fonctionne plut√¥t bien, ce qui est une bonne chose, qu'on se le dise.


## Gestion de l'√©chec d'un `Job`

Je me demande maintenant comment on g√®re les √©checs... Que se passe-t-il quand le `Job` est en √©chec ?
Dans la doc, il n'est pas indiqu√© qu'il faille faire quoi que ce soit pour g√©rer les √©checs, seulement la propri√©t√© `backoffLimit` du kind `Job` de Kubernetes qui semble permettre le rejeu en cas d'erreur.

Voyons donc comment tout √ßa va se comporter en cas d'erreur.
Pour ce faire, je modifie un peu mes scripts :
- `make-it-clap.sql` pour enregistrer le `status` en fonction de la variable d'environnement `FINAL_STATE`
- `entrypoint.sh` pour faire √©chouer le `Job` en fonction de cette m√™me variable d'environnement
- `scaled-job.yaml` pour set la variable d'environnement

Cette seconde version du batch est consultable dans [v2/make-it-clap.sql](postgres-batch/scripts/v2/make-it-clap.sql).

On relance le tout :
```zsh
$ docker build . -t postgres-batch:latest 
$ kubectl replace --force -f deploy/scaled-job.yaml
INSERT INTO make_it_clap (duration_seconds) VALUES (1);
```
Et l√†... c'est le drame !

Le job se relance bien comme indiqu√© par la propri√©t√© `.spec.jobTargetRef.template.spec.backoffLimit` :
```zsh
$ kubectl describe jobs.batch postgres-batch-vqhs7
Events:
  Type     Reason                Age   From            Message
  ----     ------                ----  ----            -------
  Normal   SuccessfulCreate      16m   job-controller  Created pod: postgres-batch-vqhs7-8p7nx
  Normal   SuccessfulCreate      16m   job-controller  Created pod: postgres-batch-vqhs7-mf4z9
  Normal   SuccessfulCreate      15m   job-controller  Created pod: postgres-batch-vqhs7-4cwdz
  Normal   SuccessfulCreate      15m   job-controller  Created pod: postgres-batch-vqhs7-cg48m
  Normal   SuccessfulCreate      14m   job-controller  Created pod: postgres-batch-vqhs7-tbfjc
  Warning  BackoffLimitExceeded  13m   job-controller  Job has reached the specified backoff limit
```
Mais il y a un "mais"... le job ne prend le `clap` √† effectuer qu'une seule fois lors de la premi√®re tentative d'execution du pod :

Premi√®re tentative :
```zsh
$ kubectl logs postgres-batch-vqhs7-8p7nx 
Running the sql script
[...]
 id | duration_seconds | id | status | session_id | creation_date | creation_time | end_time 
----+------------------+----+--------+------------+---------------+---------------+----------
  3 |                1 |    |        |            |               |               | 
(1 row)
[...]
UPDATE 1
Everything's failed
```

Seconde tentative :
```zsh
$ kubectl logs postgres-batch-vqhs7-mf4z9 
Running the sql script
[...]
 id | duration_seconds | id | status | session_id | creation_date | creation_time | end_time 
----+------------------+----+--------+------------+---------------+---------------+----------
(0 rows)
[...]
UPDATE 0
Everything's failed
```

Il faudrait donc que le batch soit fault-tolerant afin de reprendre le `clap` ayant √©chou√© ü§î.

La premi√®re solution qui me vient en t√™te est de modifier la requ√™te de pilotage du scaling pour inclure les `claps` au status `FAILED` mais cela risque fort d'engendrer un scaling infinie si le `clap` ne passe jamais √† `SUCCESS` pour quelque raison que ce soit.
Ce qui, vous le conviendrait, serait une petite cata pour nos amis les FinOps.

Une approche trop simpliste pourrait √©galement rendre le debugging (en phase de RUN) tr√®s complexe car un `Job` pourrait se mettre √† traiter des `claps` diff√©rents entre 2 retry.

Je ne sais pas si c'est la meilleure des solutions, mais c'est une solution qui a fonctionn√©e dans le cas d'un script PostgreSQL relativement simple.
Je n'ai finalement pas modifi√© la requ√™te SQL de pilotage du scaling mais j'ai pas mal modifi√© le script sql pour affecter le nom du `Job` au `clap` √† traiter, ainsi d'un retry √† l'autre, ce sera le m√™me `clap` qui sera effectu√© pour un `Job` donn√©.

Et pour bien voir les changements de status, le script √©chouera les 3 premi√®res fois pour ensuite aboutir la 4√®me fois.

On re-relance le tout :
```zsh
$ docker build . -t postgres-batch:latest 
$ kubectl replace --force -f deploy/scaled-job.yaml
INSERT INTO make_it_clap (duration_seconds) VALUES (10);
```

on scrute la cr√©ation des pods :
```zsh
$ kubectl get pods
NAME                           READY   STATUS      RESTARTS   AGE
my-release-postgresql-0        1/1     Running     0          3h12m
my-release-postgresql-client   1/1     Running     0          97m
postgres-batch-mqmg7-6d7bb     0/1     Error       0          48s
postgres-batch-mqmg7-gsj54     0/1     Error       0          81s
postgres-batch-mqmg7-h2z24     0/1     Error       0          69s
postgres-batch-mqmg7-v9hqc     0/1     Completed   0          28s
```
Et on voit que 3 tentatives sont en erreur alors que la 4√©me s'est pass√©e correctement.

En scrutant les `claps` dans postgres pendant l'execution, on voit les changements de status ainsi que le nombre d'it√©rations se mettre √† jour :
```zsh
postgres=# select * from clap;
 id | status  | session_id | creation_date | creation_time  |    end_time     | iteration
----+---------+------------+---------------+----------------+-----------------+-----------
  1 | FAILED  |  650018425 | 2021-08-04    | 01:53:09.68744 | 01:53:19.693264 |         1
(1 row)
...
  1 | RUNNING |   14628382 | 2021-08-04    | 01:53:09.68744 | 01:53:19.693264 |         1
(1 row)
...
  1 | FAILED  |   14628382 | 2021-08-04    | 01:53:09.68744 | 01:53:31.482351 |         2
(1 row)
...
  1 | RUNNING |   66819766 | 2021-08-04    | 01:53:09.68744 | 01:53:31.482351 |         2
(1 row)
...
  1 | FAILED  |   66819766 | 2021-08-04    | 01:53:09.68744 | 01:53:52.541976 |         3
(1 row)
...
  1 | RUNNING |  799850712 | 2021-08-04    | 01:53:09.68744 | 01:53:52.541976 |         3
(1 row)
...
  1 | SUCCESS |  799850712 | 2021-08-04    | 01:53:09.68744 | 01:54:12.601349 |         4
(1 row)
```

Pour √™tre tout √† fait honn√™te, je ne suis pas vraiment satisfait de la solution finale qui consiste √† mettre le nom du `Job` dans la table `make-it-clap`.

## Conclusion

Biensur, je n'ai explor√© l√† que la partie batch sur base du Scaler PostgreSQL et il est possible de scale d'autres composants Kubernetes en utilisant d'autres Scalers, 
mais Keda semble d√©j√† √™tre une bonne solution de scaling en fonction de m√©trique autre que les CPU et autre consommation m√©moire.
Il vient avec son lot de contraintes mais elles sont surmontables et surtout incontournables quand on con√ßoit un tel syst√®me tol√©rant aux pannes.
Il faut toujours garder en t√™te que certaines taches risquent d'√©chouer en concid√©rant qu'elles vont √©chouer √† coup s√ªr.

Le fait de tout devoir g√©rer manuellement pour rendre les Jobs tol√©rants aux pannes est tr√®s probablement d√ª au fait que je me soit limit√© √† utiliser des scripts SQL pour faire simple.
Je suis convaincu qu'en utilisant des outils/frameworks, comme Spring Batch par exemple, tout se passerait beaucoup mieux.

Je ferais dans un prochain billet la m√™me experience avec ce framework pour challenger cette conclusion.


.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

